<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BPS固定混合モデルの詳細説明</title>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
        });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <style>
        body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.6; margin: 20px; background-color: #f4f4f4; color: #333; }
        h1, h2, h3 { color: #0056b3; }
        .container { max-width: 900px; margin: auto; background: #fff; padding: 30px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
        .math { background-color: #e9f5ff; padding: 10px; border-radius: 5px; margin-bottom: 15px; overflow-x: auto; }
        pre { background-color: #eee; padding: 10px; border-radius: 5px; overflow-x: auto; }
        code { font-family: 'Consolas', 'Courier New', monospace; background-color: #e0e0e0; padding: 2px 4px; border-radius: 3px; }
    </style>
</head>
<body>
    <div class="container">
        <h1>BPS固定混合モデル (BPS Fixed-Mixture Model) の詳細</h1>

        <p>
            このドキュメントでは、提案されたBPS固定混合モデルの数理的な詳細を説明します。
            このモデルは、隠れマルコフモデル (HMM) の遷移則を学習しつつ、観測尤度を事前に学習済みのベースラインモデル（3因子モデルと1因子モデル）から計算するという、ハイブリッドなアプローチを採用しています。
            これにより、HMMによる柔軟な状態遷移と、安定した観測尤度計算を両立させます。
        </p>

        <h2>モデルの概要</h2>
        <p>
            モデルは2つの状態を持ち、各状態は異なる事前学習済みモデルの尤度に対応します。
            具体的には、状態1を3因子モデル（複雑な市場環境）、状態2を1因子モデル（シンプルな市場環境）に対応させます。
            HMMは、これらの状態間を動的に遷移し、観測データ $Y_t$ に基づいて現在の状態の確率 $P(S_t | Y_{1:t})$ をフィルタリングによって推定します。
        </p>

        <h3>モデルの構成要素:</h3>
        <ul>
            <li><strong>状態 ($S_t$)</strong>: 時刻 $t$ における市場の状態（1または2）。
                <ul>
                    <li>$S_t = 1$: 3因子モデルが市場を支配している状態</li>
                    <li>$S_t = 2$: 1因子モデルが市場を支配している状態</li>
                </ul>
            </li>
            <li><strong>観測データ ($Y_t$)</strong>: 時刻 $t$ における観測されたデータ。</li>
            <li><strong>外部変数 ($\eta_t^{3fac}$)</strong>: 3因子モデルから得られる潜在変数（例: 因子値）。これがHMMの遷移確率に影響を与えます。</li>
        </ul>

        <h2>数理モデルの詳細</h2>

        <h3>1. 遷移モデル (Transition Model)</h3>
        <p>
            状態遷移は、時刻 $t-1$ の状態 $S_{t-1}$ と、3因子モデルの潜在変数 $\eta_{t-1}^{3fac}$ に依存して決まります。
            特に、状態1から状態1へ遷移する確率 $P(S_t=1 | S_{t-1}=1, \eta_{t-1}^{3fac})$ は、ロジスティック関数を用いて動的に計算されます。
        </p>
        <div class="math">
            $P(S_t=1 | S_{t-1}=1, \eta_{t-1}^{3fac}) = \text{sigmoid}(\gamma_0 + \sum_{j=1}^{K} \gamma_j \eta_{j, t-1}^{3fac})$
        </div>
        <div class="math">
            $P(S_t=1 | S_{t-1}=1, \eta_{t-1}^{3fac}) = \text{sigmoid}(\gamma_0 + \boldsymbol{\gamma}_{\eta}^T \boldsymbol{\eta}_{t-1}^{3fac})$
        </div>
        <p>ここで、</p>
        <ul>
            <li>$\gamma_0$: 切片パラメータ</li>
            <li>$\boldsymbol{\gamma}_{\eta} = [\gamma_1, \dots, \gamma_K]^T$: 各潜在変数 $\eta_j^{3fac}$ に対する係数ベクトル（$K$は3因子モデルの因子数）</li>
            <li>$\boldsymbol{\eta}_{t-1}^{3fac}$: 時刻 $t-1$ における3因子モデルの潜在変数ベクトル</li>
            <li>$\text{sigmoid}(x) = \frac{1}{1 + e^{-x}}$</li>
        </ul>
        <p>その他の遷移確率は以下のように定義されます。</p>
        <div class="math">
            $P(S_t=2 | S_{t-1}=1, \eta_{t-1}^{3fac}) = 1 - P(S_t=1 | S_{t-1}=1, \eta_{t-1}^{3fac})$
        </div>
        <p>状態2から状態2へ遷移する確率は、モデルを安定させるため、高い固定値 $p_{22}$ を用います。例えば、$0.9999$。</p>
        <div class="math">
            $P(S_t=2 | S_{t-1}=2) = p_{22}$ (固定値, 例: 0.9999)
        </div>
        <div class="math">
            $P(S_t=1 | S_{t-1}=2) = 1 - p_{22}$
        </div>
        <p>これらの遷移確率を行列形式で表すと、以下のようになります（対数表現）。</p>
        <div class="math">
            $\log \mathbf{T}_t = \begin{pmatrix}
            \log P(S_t=1 | S_{t-1}=1) & \log P(S_t=2 | S_{t-1}=1) \\
            \log P(S_t=1 | S_{t-1}=2) & \log P(S_t=2 | S_{t-1}=2)
            \end{pmatrix}$
        </div>
        <p>
            ここで、学習対象のパラメータは $\gamma_0$ と $\boldsymbol{\gamma}_{\eta}$ のみです。
        </p>
        <pre><code class="language-python">
# Pyroのサンプルとして定義される学習対象パラメータ
gamma0 = pyro.sample("gamma0", dist.HalfNormal(1.0)) # あるいはinformative prior
gamma_eta = pyro.sample("gamma_eta", dist.Normal(0.0, 1.0).to_event(1)) # あるいはinformative prior

# 遷移確率の計算例
logit_p11_t = gamma0 + (eta_t_minus_1 * gamma_eta).sum(-1)
log_p11_t = F.logsigmoid(logit_p11_t)
log_p12_t = F.logsigmoid(-logit_p11_t)
log_p22 = torch.log(torch.tensor(0.9999, device=device))
log_p21 = torch.log(1.0 - torch.tensor(0.9999, device=device))

row1 = torch.stack([log_p11_t, log_p12_t], dim=-1)
row2 = torch.stack([log_p21, log_p22], dim=-1).expand(N, -1) # Nは個体数
log_transition_matrix = torch.stack([row1, row2], dim=1)
        </code></pre>

        <h3>2. 観測モデル (Observation Model)</h3>
        <p>
            このモデルの最大の特徴は、観測尤度 $P(Y_t | S_t)$ が事前に学習済みのベースラインモデルから与えられる点です。
            HMMは、この観測尤度を学習するのではなく、**所与の事実**として利用します。
        </p>
        <ul>
            <li>$P(Y_t | S_t=1)$: 事前学習済み3因子モデルの対数尤度 $\log P(Y_t | \text{3-factor model})$</li>
            <li>$P(Y_t | S_t=2)$: 事前学習済み1因子モデルの対数尤度 $\log P(Y_t | \text{1-factor model})$</li>
        </ul>
        <div class="math">
            $\log P(Y_t | S_t=1) = \text{LogLik}_{3fac}(Y_t)$
        </div>
        <div class="math">
            $\log P(Y_t | S_t=2) = \text{LogLik}_{1fac}(Y_t)$
        </div>
        <p>これらの対数尤度は、カルマンフィルタリングのアルゴリズムによって各時刻で計算されます。</p>
        <pre><code class="language-python">
# ベースラインモデルからの対数尤度を事前に計算
log_lik_3fac_train = get_kalman_log_likelihoods_per_step(Y_train, ...)
log_lik_1fac_train = get_kalman_log_likelihoods_per_step(Y_train, ...)

# HMM内で使用
log_liks_t = torch.stack([log_lik_3fac[:, t], log_lik_1fac[:, t]], dim=-1)
        </code></pre>

        <h3>3. フィルタリングと学習 (Filtering and Learning)</h3>
        <p>
            HMMのフィルタリングは、予測ステップと更新ステップを繰り返すことで、各時刻における状態確率 $P(S_t | Y_{1:t})$ を計算します。
        </p>
        <h4>予測ステップ:</h4>
        <p>
            前の時刻までの信念 $P(S_{t-1} | Y_{1:t-1})$ と遷移確率 $P(S_t | S_{t-1})$ を用いて、現在の時刻の状態の事前確率 $P(S_t | Y_{1:t-1})$ を計算します。
        </p>
        <div class="math">
            $P(S_t | Y_{1:t-1}) = \sum_{S_{t-1}} P(S_t | S_{t-1}) P(S_{t-1} | Y_{1:t-1})$
        </div>
        <p>対数領域では $\log P(S_t | Y_{1:t-1}) = \text{logsumexp}(\log P(S_{t-1} | Y_{1:t-1}) + \log P(S_t | S_{t-1}))$ となります。</p>

        <h4>更新ステップ:</h4>
        <p>
            現在の時刻の状態の事前確率 $P(S_t | Y_{1:t-1})$ と、観測尤度 $P(Y_t | S_t)$ を用いて、現在の時刻の状態の事後確率 $P(S_t | Y_{1:t})$ を計算します。
        </p>
        <div class="math">
            $P(S_t | Y_{1:t}) = \frac{P(Y_t | S_t) P(S_t | Y_{1:t-1})}{P(Y_t | Y_{1:t-1})}$
        </div>
        <p>ここで、混合対数尤度 $P(Y_t | Y_{1:t-1})$ は正規化定数として機能し、最大化すべき目的関数の一部となります。</p>
        <div class="math">
            $P(Y_t | Y_{1:t-1}) = \sum_{S_t} P(Y_t | S_t) P(S_t | Y_{1:t-1})$
        </div>
        <p>この目的関数をPyroの変分推論 (Variational Inference, VI) を用いて最大化することで、遷移パラメータ $\gamma_0$ と $\boldsymbol{\gamma}_{\eta}$ が学習されます。</p>
        <pre><code class="language-python">
# 予測信念の更新
log_beliefs = torch.logsumexp(log_beliefs.unsqueeze(2) + log_transition_matrix, dim=1)

# 観測尤度
log_liks_t = torch.stack([log_lik_3fac[:, t], log_lik_1fac[:, t]], dim=-1)

# 混合対数尤度（これが目的関数に寄与）
marginal_log_lik = torch.logsumexp(log_beliefs + log_liks_t, dim=-1)
pyro.factor(f"obs_log_prob_{t}", marginal_log_lik)

# 更新後信念
log_beliefs = (log_beliefs + log_liks_t) - marginal_log_lik.unsqueeze(-1)
        </code></pre>

        <h2>モデルの利点</h2>
        <ul>
            <li>
                <strong>高速な学習と安定性:</strong> 観測モデルのパラメータを再学習しないため、従来のレジームスイッチングモデルと比較して学習が高速かつ安定します。
            </li>
            <li>
                <strong>データ駆動型の遷移:</strong> HMMの遷移確率が外部変数 ($\eta^{3fac}$) に依存するため、市場の動的な特徴に基づいて状態が切り替わります。
            </li>
            <li>
                <strong>生のデータへの対応:</strong> 因子モデルの尤度を直接使用することで、データドリフトモデルのように中間変数（例: `data_drift`）を学習する必要がありません。
            </li>
            <li>
                <strong>柔軟な予測:</strong> 異なる因子モデルの予測値を重み付けすることで、よりロバストな予測が可能になります。
            </li>
        </ul>

        <h2>学習と評価</h2>
        <p>
            PyTorchとPyroのSVI（Stochastic Variational Inference）を用いて、`gamma0`と`gamma_eta`の事後分布を推定します。
            モデルの評価は、学習済みパラメータを用いてテストデータに対するRMSE（Root Mean Squared Error）、感度（Sensitivity）、特異度（Specificity）、および潜在変数$\eta$のSSR（Sum of Squared Residuals）を計算することで行われます。
        </p>
    </div>
</body>
</html>