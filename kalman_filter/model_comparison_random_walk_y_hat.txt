--- 0. Defining Parameters for the 3-to-1 Factor Regime-Switching Model ---
Using device: cpu
Running in-sample evaluation mode (no train-test split).

--- 1. Generating Simulation Data ---
Loading pre-computed simulation data from 'simulation_data_rw.pt'...
Data loading complete.

--- 2. Defining Common Functions ---

--- 3. Estimation of Baseline Models ---

--- 3a. 3-Factor Model ---
  [Epoch 0100] loss: 47912.8438 (Best: 47932.1445)
  [Epoch 0200] loss: 46171.9961 (Best: 46187.9727)
  [Epoch 0300] loss: 44754.5703 (Best: 44767.1055)
  [Epoch 0400] loss: 43587.5586 (Best: 43598.4961)
  [Epoch 0500] loss: 42515.2070 (Best: 42525.7891)
  [Epoch 0600] loss: 41666.6289 (Best: 41673.7734)
  [Epoch 0700] loss: 41028.7266 (Best: 41034.4219)
  [Epoch 0800] loss: 40522.3828 (Best: 40526.9062)
  [Epoch 0900] loss: 40114.4844 (Best: 40118.1836)
  [Epoch 1000] loss: 39774.3750 (Best: 39777.5234)
  [Epoch 1100] loss: 39478.6094 (Best: 39481.4180)
  [Epoch 1200] loss: 39215.1523 (Best: 39217.6211)
  [Epoch 1300] loss: 38983.4062 (Best: 38985.5898)
  [Epoch 1400] loss: 38777.7891 (Best: 38779.7305)
  [Epoch 1500] loss: 38595.2578 (Best: 38596.9727)
  [Epoch 1600] loss: 38433.4922 (Best: 38435.0156)
  [Epoch 1700] loss: 38291.0078 (Best: 38292.3438)
  [Epoch 1800] loss: 38166.8594 (Best: 38168.0156)
  [Epoch 1900] loss: 38060.4766 (Best: 38061.4570)
  [Epoch 2000] loss: 37971.1719 (Best: 37971.9805)
  [Epoch 2100] loss: 37897.9297 (Best: 37898.5820)
  [Epoch 2200] loss: 37839.2539 (Best: 37839.7734)
  [Epoch 2300] loss: 37793.3281 (Best: 37793.7344)
  [Epoch 2400] loss: 37758.1875 (Best: 37758.4883)
  [Epoch 2500] loss: 37731.8594 (Best: 37732.0898)
  [Epoch 2600] loss: 37712.5625 (Best: 37712.7227)
  [Epoch 2700] loss: 37698.7188 (Best: 37698.8398)
  [Epoch 2800] loss: 37688.9883 (Best: 37689.0703)
  [Epoch 2900] loss: 37682.2656 (Best: 37682.3203)
  [Epoch 3000] loss: 37677.6797 (Best: 37677.7266)
  [Epoch 3100] loss: 37674.5781 (Best: 37674.6016)
  [Epoch 3200] loss: 37672.4727 (Best: 37672.4805)
  [Epoch 3300] loss: 37671.0195 (Best: 37671.0312)
  [Epoch 3400] loss: 37669.9961 (Best: 37670.0156)
  [Epoch 3500] loss: 37669.2734 (Best: 37669.2734)
  [Epoch 3600] loss: 37668.7305 (Best: 37668.7266)
  [Epoch 3700] loss: 37668.3164 (Best: 37668.3242)
  [Epoch 3800] loss: 37667.9961 (Best: 37668.0000)
  [Epoch 3900] loss: 37667.7539 (Best: 37667.7578)
  [Epoch 4000] loss: 37667.5664 (Best: 37667.5625)
  [Epoch 4100] loss: 37667.4180 (Best: 37667.4180)
  [Epoch 4200] loss: 37667.3047 (Best: 37667.3047)
  [Epoch 4300] loss: 37667.2188 (Best: 37667.2148)
  [Epoch 4400] loss: 37667.1523 (Best: 37667.1484)
  [Epoch 4500] loss: 37667.1016 (Best: 37667.1016)
  [Epoch 4600] loss: 37667.0664 (Best: 37667.0625)
  [Epoch 4700] loss: 37667.0430 (Best: 37667.0391)
  [Epoch 4800] loss: 37667.0195 (Best: 37667.0234)
  [Epoch 4900] loss: 37667.0156 (Best: 37667.0078)
  [Epoch 5000] loss: 37667.0000 (Best: 37667.0000)
  [Epoch 5100] loss: 37666.9922 (Best: 37666.9922)
  [Epoch 5200] loss: 37666.9922 (Best: 37666.9883)
    -> Early stopping triggered at epoch 5272.
Saving 3-factor model results to 'baseline_3fac_results.pt'...
Saving complete.

--- 3b. 1-Factor Model ---
  [Epoch 0100] loss: 46178.2383 (Best: 46188.9922)
  [Epoch 0200] loss: 45221.2891 (Best: 45229.8008)
  [Epoch 0300] loss: 44479.5898 (Best: 44486.1250)
  [Epoch 0400] loss: 43879.8945 (Best: 43885.2852)
  [Epoch 0500] loss: 43390.0938 (Best: 43394.5352)
  [Epoch 0600] loss: 42986.1250 (Best: 42989.7969)
  [Epoch 0700] loss: 42649.1484 (Best: 42652.2305)
  [Epoch 0800] loss: 42363.9141 (Best: 42366.5352)
  [Epoch 0900] loss: 42120.6523 (Best: 42122.9023)
  [Epoch 1000] loss: 41911.7227 (Best: 41913.6641)
  [Epoch 1100] loss: 41731.0781 (Best: 41732.7617)
  [Epoch 1200] loss: 41572.8828 (Best: 41574.3711)
  [Epoch 1300] loss: 41429.7344 (Best: 41431.1328)
  [Epoch 1400] loss: 41287.3477 (Best: 41288.8750)
  [Epoch 1500] loss: 41050.1211 (Best: 41055.3125)
  [Epoch 1600] loss: 40953.3164 (Best: 40953.6719)
  [Epoch 1700] loss: 40921.8750 (Best: 40922.1523)
  [Epoch 1800] loss: 40897.1484 (Best: 40897.3711)
  [Epoch 1900] loss: 40877.3867 (Best: 40877.5703)
  [Epoch 2000] loss: 40861.2188 (Best: 40861.3672)
  [Epoch 2100] loss: 40847.6328 (Best: 40847.7617)
  [Epoch 2200] loss: 40835.8398 (Best: 40835.9531)
  [Epoch 2300] loss: 40825.2891 (Best: 40825.3828)
  [Epoch 2400] loss: 40815.5508 (Best: 40815.6523)
  [Epoch 2500] loss: 40806.3594 (Best: 40806.4492)
  [Epoch 2600] loss: 40797.5195 (Best: 40797.6016)
  [Epoch 2700] loss: 40788.8984 (Best: 40788.9844)
  [Epoch 2800] loss: 40780.4375 (Best: 40780.5156)
  [Epoch 2900] loss: 40772.1016 (Best: 40772.1914)
  [Epoch 3000] loss: 40763.9023 (Best: 40763.9844)
  [Epoch 3100] loss: 40755.8320 (Best: 40755.9102)
  [Epoch 3200] loss: 40747.9375 (Best: 40748.0117)
  [Epoch 3300] loss: 40740.2188 (Best: 40740.2930)
  [Epoch 3400] loss: 40732.7227 (Best: 40732.8008)
  [Epoch 3500] loss: 40725.4648 (Best: 40725.5312)
  [Epoch 3600] loss: 40718.4805 (Best: 40718.5508)
  [Epoch 3700] loss: 40711.7734 (Best: 40711.8398)
  [Epoch 3800] loss: 40705.3711 (Best: 40705.4336)
  [Epoch 3900] loss: 40699.2734 (Best: 40699.3359)
  [Epoch 4000] loss: 40693.4883 (Best: 40693.5430)
  [Epoch 4100] loss: 40688.0156 (Best: 40688.0664)
  [Epoch 4200] loss: 40682.8398 (Best: 40682.8867)
  [Epoch 4300] loss: 40677.9492 (Best: 40678.0039)
  [Epoch 4400] loss: 40673.3516 (Best: 40673.3906)
  [Epoch 4500] loss: 40669.0078 (Best: 40669.0547)
  [Epoch 4600] loss: 40664.9375 (Best: 40664.9766)
  [Epoch 4700] loss: 40661.1055 (Best: 40661.1367)
  [Epoch 4800] loss: 40657.4961 (Best: 40657.5312)
  [Epoch 4900] loss: 40654.1094 (Best: 40654.1406)
  [Epoch 5000] loss: 40650.9219 (Best: 40650.9531)
  [Epoch 5100] loss: 40647.9219 (Best: 40647.9453)
  [Epoch 5200] loss: 40645.1016 (Best: 40645.1289)
  [Epoch 5300] loss: 40642.4453 (Best: 40642.4727)
  [Epoch 5400] loss: 40639.9414 (Best: 40639.9648)
  [Epoch 5500] loss: 40637.5859 (Best: 40637.6094)
  [Epoch 5600] loss: 40635.3594 (Best: 40635.3828)
  [Epoch 5700] loss: 40633.2656 (Best: 40633.2852)
  [Epoch 5800] loss: 40631.2891 (Best: 40631.3047)
  [Epoch 5900] loss: 40629.4180 (Best: 40629.4414)
  [Epoch 6000] loss: 40627.6484 (Best: 40627.6641)
  [Epoch 6100] loss: 40625.9805 (Best: 40625.9883)
  [Epoch 6200] loss: 40624.3906 (Best: 40624.4062)
  [Epoch 6300] loss: 40622.8867 (Best: 40622.8984)
  [Epoch 6400] loss: 40621.4688 (Best: 40621.4844)
  [Epoch 6500] loss: 40620.1172 (Best: 40620.1328)
  [Epoch 6600] loss: 40618.8359 (Best: 40618.8477)
  [Epoch 6700] loss: 40617.6211 (Best: 40617.6328)
  [Epoch 6800] loss: 40616.4688 (Best: 40616.4766)
  [Epoch 6900] loss: 40615.3750 (Best: 40615.3789)
  [Epoch 7000] loss: 40614.3242 (Best: 40614.3359)
  [Epoch 7100] loss: 40613.3398 (Best: 40613.3477)
  [Epoch 7200] loss: 40612.3945 (Best: 40612.3984)
  [Epoch 7300] loss: 40611.4922 (Best: 40611.5000)
  [Epoch 7400] loss: 40610.6289 (Best: 40610.6406)
  [Epoch 7500] loss: 40609.8125 (Best: 40609.8203)
  [Epoch 7600] loss: 40609.0391 (Best: 40609.0430)
  [Epoch 7700] loss: 40608.2852 (Best: 40608.2930)
  [Epoch 7800] loss: 40607.5781 (Best: 40607.5820)
  [Epoch 7900] loss: 40606.9023 (Best: 40606.9102)
  [Epoch 8000] loss: 40606.2539 (Best: 40606.2617)
  [Epoch 8100] loss: 40605.6328 (Best: 40605.6367)
  [Epoch 8200] loss: 40605.0391 (Best: 40605.0430)
  [Epoch 8300] loss: 40604.4727 (Best: 40604.4766)
  [Epoch 8400] loss: 40603.9336 (Best: 40603.9414)
  [Epoch 8500] loss: 40603.4141 (Best: 40603.4141)
  [Epoch 8600] loss: 40602.9180 (Best: 40602.9180)
  [Epoch 8700] loss: 40602.4414 (Best: 40602.4453)
  [Epoch 8800] loss: 40601.9883 (Best: 40601.9961)
  [Epoch 8900] loss: 40601.5547 (Best: 40601.5586)
  [Epoch 9000] loss: 40601.1328 (Best: 40601.1367)
  [Epoch 9100] loss: 40600.7383 (Best: 40600.7461)
  [Epoch 9200] loss: 40600.3516 (Best: 40600.3594)
  [Epoch 9300] loss: 40599.9883 (Best: 40599.9883)
  [Epoch 9400] loss: 40599.6406 (Best: 40599.6406)
  [Epoch 9500] loss: 40599.2930 (Best: 40599.2969)
  [Epoch 9600] loss: 40598.9766 (Best: 40598.9844)
  [Epoch 9700] loss: 40598.6602 (Best: 40598.6641)
  [Epoch 9800] loss: 40598.3672 (Best: 40598.3672)
  [Epoch 9900] loss: 40598.0820 (Best: 40598.0859)
  [Epoch 10000] loss: 40597.8125 (Best: 40597.8125)
Saving 1-factor model results to 'baseline_1fac_results.pt'...
Saving complete.


--- 4. Defining and Training BPS Random-Walk Model ---
Pre-calculating inputs for BPS model (using predicted y_hat)...
Pre-calculation complete.
Starting BPS model training with VI...
