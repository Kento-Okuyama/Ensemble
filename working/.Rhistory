}
}
}
generated quantities {
real weighted_prediction;
real variance_weighted;
matrix[N, Nt-1] y_rep;             // Posterior predictive samples
matrix[N, Nt-1] log_lik;           // Log-likelihood for LOO
for (n in 1:N) {
for (t in 2:Nt) {
real target_mean = lambda * dot_product(w[t], mu) + (1 - lambda) * y[n, t - 1];
vector[4] y_pred;
y_pred[1] = (1 - ar_pos) * target_mean + ar_pos * y[n, t-1];
y_pred[2] = (1 - ar_neg) * target_mean + ar_neg * y[n, t-1];
y_pred[3] = target_mean + ma_coef * (y[n, t-1] - target_mean);
y_pred[4] = target_mean;
weighted_prediction = dot_product(w[t], y_pred);
variance_weighted = dot_product(w[t], sigma);
y_rep[n, t-1] = normal_rng(weighted_prediction, variance_weighted);
log_lik[n, t-1] = normal_lpdf(y[n, t] | weighted_prediction, variance_weighted);
}
}
}
"
# Compile and sample from the Stan model
fit <- stan(model_code = stan_code, data = stan_data, iter = 2000, chains = 4)
# Compile and sample the Stan model
# fit <- stan(
#   model_code = stan_code,
#   data = stan_data,
#   iter = 4000,             # Increase number of iterations to improve convergence and ESS
#   chains = 4,
#   control = list(
#     adapt_delta = 0.99,    # Set adapt_delta to 0.99 to reduce divergent transitions
#     max_treedepth = 15     # Increase max_treedepth to 15 for better exploration of parameter space
#   )
# )
# Extract log-likelihood for PSIS-LOO cross-validation
log_lik <- extract_log_lik(fit, parameter_name = "log_lik", merge_chains = FALSE)
loo_result <- loo(log_lik, moment_match = TRUE)
print(loo_result)
# Posterior analysis of dynamic weights
bayesplot::mcmc_areas(as.array(fit), pars = c("ar_pos", "ar_neg", "ma_coef")) +
labs(title = "Posterior distributions of AR and MA coefficients")
bayesplot::mcmc_areas(as.array(fit), pars = c("sigma[1]", "sigma[2]", "sigma[3]", "sigma[4]")) +
labs(title = "Posterior distributions of state standard deviations")
bayesplot::mcmc_areas(as.array(fit), pars = c("mu[1]", "mu[2]", "mu[3]", "mu[4]", "lambda")) +
labs(title = "Posterior distributions of state means and smoothing parameter")
bayesplot::mcmc_areas(as.array(fit), pars = c("T[1,1]", "T[2,2]", "T[3,3]", "T[4,4]")) +
labs(title = "Posterior distributions of transition matrix diagonal elements")
# Check posterior distribution of transition matrix
print(fit, pars = "T")
# Create a time index for 51 time points
time_index <- 1:51
# Define the indices of the individuals to include in the plot
person_index <- c(20, 24, 38)
# Attach the filtered_best object to access its variables directly
attach(filtered_best)
# Initialize an empty data frame to hold the data for plotting
data <- data.frame()
# Loop through each individual and extract their data
for (person in person_index) {
# Combine data for each individual into a single data frame
data <- rbind(data, data.frame(
Time = rep(time_index, times = 4), # Repeat time indices for all 4 variables (Series)
Value = c(eta1_best[person, , 1], eta1_best[person, , 2], eta1_best[person, , 3], eta1_best[person, , 4]), # Extract values for each variable
StdDev = sqrt(c(P_best[person, , 1, 1], P_best[person, , 2, 2], P_best[person, , 3, 3], P_best[person, , 4, 4])), # Compute standard deviations for error bands
Series = factor(rep(1:4, each = 51)), # Label each variable as Series 1 to 4
Person = factor(rep(person, times = 51 * 4)) # Label each individual by their ID
))
}
# Detach the filtered_best object after extracting all necessary data
detach(filtered_best)
# Update the Series labels with descriptive names
series_labels <- c("Cost", "Not Understanding", "Afraid to Fail", "Negative Affect")
# Plot the data using ggplot
ggplot(data, aes(x = Time, y = Value, color = Person, fill = Person)) +
geom_line(size = 1) + # Plot lines for each individual
geom_ribbon(aes(ymin = Value - StdDev, ymax = Value + StdDev), alpha = 0.2) + # Add error bands using standard deviation
facet_wrap(~ Series, scales = "free_y", labeller = labeller(Series = setNames(series_labels, 1:4))) + # Separate plots for each variable with descriptive labels
labs(
title = "Time Series with Variance Bands for Multiple Individuals", # Title of the plot
x = "Time Index", # Label for the x-axis
y = "Value" # Label for the y-axis
) +
theme_minimal() + # Use a minimal theme for a clean look
theme(
legend.title = element_blank(), # Remove the title of the legend
strip.text = element_text(size = 12) # Adjust the size of facet labels
)
# Create a time index for 51 time points
time_index <- 1:51
# Define the indices of the individuals to include in the plot
person_index <- c(20, 24, 38)
# Attach the filtered_best object to access its variables directly
attach(filtered_best)
# Clear the environment
rm(list = ls())
# Set seed value
seed <- 42
m <- 1
# Set working directory
setwd('C:/Users/Methodenzentrum/Desktop/Kento/Empirical/20241025 - Copy')
# Load necessary libraries
library(rstan)
library(dplyr)
library(ggplot2)
library(loo)
# Data generation parameters (same as before)
set.seed(123)
N <- 10    # Number of time series
Nt <- 50   # Length of each time series
lambda <- 0.5
mu <- c(2, -2, 1, 0)
ar_pos <- 0.7
ar_neg <- -0.7
ma_coef <- 0.5
sigma <- c(0.5, 0.7, 0.6, 0.4)
# Transition probability matrix (same as before)
transition_matrix <- matrix(c(
0.8, 0.05, 0.1, 0.05,
0.1, 0.8, 0.05, 0.05,
0.1, 0.05, 0.8, 0.05,
0.1, 0.05, 0.1, 0.75
), nrow = 4, byrow = TRUE)
# Data generation function (same as before)
generate_data <- function(N, Nt, mu, sigma, transition_matrix, lambda, ar_pos, ar_neg, ma_coef) {
y <- matrix(NA, nrow = N, ncol = Nt)
regimes <- matrix(NA, nrow = N, ncol = Nt)
for (n in 1:N) {
current_regime <- 1
y[n, 1] <- rnorm(1, mean = mu[current_regime], sd = sigma[current_regime])
regimes[n, 1] <- current_regime
for (t in 2:Nt) {
current_regime <- sample(1:4, 1, prob = transition_matrix[current_regime, ])
regimes[n, t] <- current_regime
target_mean <- lambda * mu[current_regime] + (1 - lambda) * y[n, t - 1]
if (current_regime == 1) {
y[n, t] <- (1 - ar_pos) * target_mean + ar_pos * y[n, t-1] + rnorm(1, sd = sigma[current_regime])
} else if (current_regime == 2) {
y[n, t] <- (1 - ar_neg) * target_mean + ar_neg * y[n, t-1] + rnorm(1, sd = sigma[current_regime])
} else if (current_regime == 3) {
y[n, t] <- target_mean + ma_coef * (y[n, t - 1] - target_mean) + rnorm(1, sd = sigma[current_regime])
} else {
y[n, t] <- rnorm(1, mean = target_mean, sd = sigma[current_regime])
}
}
}
list(y = y, regimes = regimes)
}
# Generate data
data <- generate_data(N, Nt, mu, sigma, transition_matrix, lambda, ar_pos, ar_neg, ma_coef)
y <- data$y
# Prepare data for Stan
stan_data <- list(
N = N,
Nt = Nt,
y = y
)
# Stan model code for individual model fitting
stan_individual_code <- "
data {
int<lower=1> N;          // Number of time series
int<lower=1> Nt;         // Length of each time series
real y[N, Nt];           // Observed data
}
parameters {
real<lower=0, upper=1> ar_coef;     // AR coefficient
real<lower=-1, upper=1> ma_coef;    // MA coefficient
real mu;                            // Mean for the white noise model
real<lower=0> sigma;                // Standard deviation for all models
}
model {
// Priors
ar_coef ~ beta(2, 2);       // Prior for AR coefficient
ma_coef ~ normal(0, 0.5);   // Prior for MA coefficient
mu ~ normal(0, 5);          // Prior for mean
sigma ~ cauchy(0, 2);       // Prior for standard deviation
// Likelihood
for (n in 1:N) {
for (t in 2:Nt) {
real prediction_ar = ar_coef * y[n, t - 1];
real prediction_ma = mu + ma_coef * (y[n, t - 1] - mu);
real prediction_wn = mu;
// Add likelihoods for each model
y[n, t] ~ normal(prediction_ar, sigma);  // AR(1)
y[n, t] ~ normal(prediction_ma, sigma);  // MA(1)
y[n, t] ~ normal(prediction_wn, sigma);  // White Noise
}
}
}
generated quantities {
real y_pred_ar[N, Nt];
real y_pred_ma[N, Nt];
real y_pred_wn[N, Nt];
for (n in 1:N) {
y_pred_ar[n, 1] = mu;  // Initialize predictions
y_pred_ma[n, 1] = mu;
y_pred_wn[n, 1] = mu;
for (t in 2:Nt) {
y_pred_ar[n, t] = normal_rng(ar_coef * y[n, t - 1], sigma);
y_pred_ma[n, t] = normal_rng(mu + ma_coef * (y[n, t - 1] - mu), sigma);
y_pred_wn[n, t] = normal_rng(mu, sigma);
}
}
}
"
# Compile and fit individual models
fit_individual <- stan(model_code = stan_individual_code, data = stan_data, iter = 2000, chains = 4)
# Extract predictions from generated quantities
y_pred_ar <- extract(fit_individual, pars = "y_pred_ar")$y_pred_ar
y_pred_ma <- extract(fit_individual, pars = "y_pred_ma")$y_pred_ma
y_pred_wn <- extract(fit_individual, pars = "y_pred_wn")$y_pred_wn
# Compute mean predictions for each model
f1 <- apply(y_pred_ar, c(2, 3), mean)  # AR predictions
f2 <- apply(y_pred_ma, c(2, 3), mean)  # MA predictions
f3 <- apply(y_pred_wn, c(2, 3), mean)  # White noise predictions
# Combine predictions into a single array for BPS
f <- array(c(f1, f2, f3), dim = c(N, Nt, 3))
# Prepare data for BPS
stan_bps_data <- list(
N = N,
Nt = Nt,
J = 3,   # Number of models
y = y,
f = f
)
# Stan model code for BPS
stan_bps_code <- "
data {
int<lower=1> N;          // Number of time series
int<lower=1> Nt;         // Length of each time series
int<lower=1> J;          // Number of models
real y[N, Nt];           // Observed data
real f[N, Nt, J];        // Model predictions
}
parameters {
vector[Nt] alpha;         // Time-varying intercept
simplex[J] beta[Nt];      // Time-varying weights for each time step
real<lower=0> sigma;      // Observation noise standard deviation
}
model {
// Priors
alpha ~ normal(0, 5);
for (t in 1:Nt) {
beta[t] ~ dirichlet(rep_vector(1.0, J));  // Dirichlet prior for weights
}
sigma ~ cauchy(0, 2);
// Likelihood
for (n in 1:N) {
for (t in 1:Nt) {
vector[J] model_predictions;
for (j in 1:J) {
model_predictions[j] = f[n, t, j];
}
y[n, t] ~ normal(alpha[t] + dot_product(beta[t], model_predictions), sigma);
}
}
}
generated quantities {
matrix[N, Nt] log_lik;  // Log-likelihood for PSIS-LOO
for (n in 1:N) {
for (t in 1:Nt) {
vector[J] model_predictions;
for (j in 1:J) {
model_predictions[j] = f[n, t, j];
}
log_lik[n, t] = normal_lpdf(y[n, t] | alpha[t] + dot_product(beta[t], model_predictions), sigma);
}
}
}
"
# Compile and fit BPS model
fit_bps <- stan(model_code = stan_bps_code, data = stan_bps_data, iter = 2000, chains = 4)
# Display results
print(fit_bps, pars = c("alpha", "beta", "sigma"))
# Extract log-likelihood for PSIS-LOO cross-validation
log_lik <- extract_log_lik(fit_bps, parameter_name = "log_lik", merge_chains = FALSE)
loo_result <- loo(log_lik, moment_match = TRUE)
print(loo_result)
# Load necessary libraries
library(rstan)
library(bayesplot)
library(loo)
library(ggplot2)
library(dplyr)
# Set the seed for reproducibility
set.seed(123)
# Parameters for data simulation
N <- 10    # Number of time series
Nt <- 50  # Length of each time series
lambda <- 0.5  # Smoothing parameter for target_mean
mu <- c(2, -2, 1, 0)         # Regime means
ar_pos <- 0.7                # Positive AR coefficient
ar_neg <- -0.7               # Negative AR coefficient
ma_coef <- 0.5               # MA coefficient
sigma <- c(0.5, 0.7, 0.6, 0.4)  # Standard deviations for each regime
# Define transition matrix for regime switching
transition_matrix <- matrix(c(
0.8, 0.05, 0.1, 0.05,
0.1, 0.8, 0.05, 0.05,
0.1, 0.05, 0.8, 0.05,
0.1, 0.05, 0.1, 0.75
), nrow = 4, byrow = TRUE)
# Initialize matrices to store simulated data
y <- matrix(NA, nrow = N, ncol = Nt)
regime <- matrix(NA, nrow = N, ncol = Nt)
# Simulate data with regime transitions
for (n in 1:N) {
current_regime <- 1 # sample(1:4, 1)
y[n, 1] <- rnorm(1, mean = mu[current_regime], sd = sigma[current_regime])  # Initial value
regime[n, 1] <- current_regime
for (t in 2:Nt) {
current_regime <- sample(1:4, 1, prob = transition_matrix[current_regime, ])
regime[n, t] <- current_regime
target_mean <- lambda * mu[current_regime] + (1 - lambda) * y[n, t - 1]
if (current_regime == 1) {
y[n, t] <- (1 - ar_pos) * target_mean + ar_pos * y[n, t-1] + rnorm(1, mean = 0, sd = sigma[current_regime])
} else if (current_regime == 2) {
y[n, t] <- (1 - ar_neg) * target_mean + ar_neg * y[n, t-1] + rnorm(1, mean = 0, sd = sigma[current_regime])
} else if (current_regime == 3) {
y[n, t] <- target_mean + ma_coef * (y[n, t - 1] - target_mean) + rnorm(1, mean = 0, sd = sigma[current_regime])
} else {
y[n, t] <- rnorm(1, mean = target_mean, sd = sigma[current_regime])
}
}
}
# Convert to data frame for ggplot visualization
y_df <- data.frame(
time = rep(1:Nt, times = N),
value = as.vector(y),
series = rep(1:N, each = Nt),
regime = as.vector(regime)
)
# Plot each individual's time series with regime-based coloring
ggplot(y_df, aes(x = time, y = value, group = series, color = as.factor(regime))) +
geom_line(aes(color = as.factor(regime))) +  # Line changes color based on regime
geom_point(size = 1) +  # Add points to emphasize regime switches
facet_wrap(~ series, ncol = 1, scales = "free_y") +  # Separate plots for each individual
labs(title = "Time Series with Regime Switching for Each Individual",
color = "Regime") +
theme_minimal() +
theme(legend.position = "bottom") +
scale_color_manual(values = c("1" = "blue", "2" = "red", "3" = "green", "4" = "purple"))
# Count occurrences of each regime in the simulated data
regime_counts <- y_df %>%
group_by(regime) %>%
summarize(count = n())
# Plot the distribution of regimes
ggplot(regime_counts, aes(x = as.factor(regime), y = count, fill = as.factor(regime))) +
geom_bar(stat = "identity") +
labs(title = "Frequency of Each Regime", x = "Regime", y = "Count") +
theme_minimal()
# Prepare data list for Stan
stan_data <- list(
N = N,
Nt = Nt,
regime = regime,
y = y
)
# Stan model code
stan_code <- "
data {
int<lower=1> N;                   // Number of individuals
int<lower=1> Nt;                  // Time series length for each individual
real y[N, Nt];                    // Observed data
}
parameters {
vector[4] mu;                     // Mean for each state
real<lower=0, upper=1> lambda;    // Smoothing parameter for target mean
real<lower=0, upper=1> ar_pos;    // AR coefficient for state 1
real<lower=-1, upper=0> ar_neg;   // AR coefficient for state 2
real<lower=-1, upper=1> ma_coef;  // MA coefficient for state 3
vector<lower=0>[4] sigma;         // Standard deviation for each state
simplex[4] T[4];                  // Transition probability matrix
}
transformed parameters {
vector[4] w[Nt];               // Dynamic weights
// Initialize weights
w[1] = [1, 0, 0, 0]';
// Update weights based on transition probabilities
for (t in 2:Nt) {
for (j in 1:4) {
w[t][j] = T[1][j] * w[t - 1][1] + T[2][j] * w[t - 1][2] + T[3][j] * w[t - 1][3] + T[4][j] * w[t - 1][4];
}
}
}
model {
real weighted_prediction;
real variance_weighted;
// Prior distributions for parameters
mu ~ normal(0, 5);
lambda ~ beta(2, 2);
ar_pos ~ normal(0.7, 0.3);
ar_neg ~ normal(-0.7, 0.3);
ma_coef ~ normal(0, 0.5);
sigma ~ cauchy(0, 2);
// Priors for transition matrix
for (i in 1:4) {
vector[4] alpha = rep_vector(0.1, 4);
alpha[i] = 0.8;
T[i] ~ dirichlet(alpha);
}
// State-dependent model for each time series
for (n in 1:N) {
for (t in 2:Nt) {
real target_mean = lambda * dot_product(w[t], mu) + (1 - lambda) * y[n, t - 1];
vector[4] y_pred;
// State-specific predictions
y_pred[1] = (1 - ar_pos) * target_mean + ar_pos * y[n, t-1];
y_pred[2] = (1 - ar_neg) * target_mean + ar_neg * y[n, t-1];
y_pred[3] = target_mean + ma_coef * (y[n, t - 1] - target_mean);
y_pred[4] = target_mean;
// Weighted prediction based on dynamic weights
weighted_prediction = dot_product(w[t], y_pred);
variance_weighted = dot_product(w[t], sigma);
// Observation model
y[n, t] ~ normal(weighted_prediction, variance_weighted);
}
}
}
generated quantities {
real weighted_prediction;
real variance_weighted;
matrix[N, Nt-1] y_rep;             // Posterior predictive samples
matrix[N, Nt-1] log_lik;           // Log-likelihood for LOO
for (n in 1:N) {
for (t in 2:Nt) {
real target_mean = lambda * dot_product(w[t], mu) + (1 - lambda) * y[n, t - 1];
vector[4] y_pred;
y_pred[1] = (1 - ar_pos) * target_mean + ar_pos * y[n, t-1];
y_pred[2] = (1 - ar_neg) * target_mean + ar_neg * y[n, t-1];
y_pred[3] = target_mean + ma_coef * (y[n, t-1] - target_mean);
y_pred[4] = target_mean;
weighted_prediction = dot_product(w[t], y_pred);
variance_weighted = dot_product(w[t], sigma);
y_rep[n, t-1] = normal_rng(weighted_prediction, variance_weighted);
log_lik[n, t-1] = normal_lpdf(y[n, t] | weighted_prediction, variance_weighted);
}
}
}
"
# Compile and sample from the Stan model
fit <- stan(model_code = stan_code, data = stan_data, iter = 2000, chains = 4)
# Compile and sample the Stan model
# fit <- stan(
#   model_code = stan_code,
#   data = stan_data,
#   iter = 4000,             # Increase number of iterations to improve convergence and ESS
#   chains = 4,
#   control = list(
#     adapt_delta = 0.99,    # Set adapt_delta to 0.99 to reduce divergent transitions
#     max_treedepth = 15     # Increase max_treedepth to 15 for better exploration of parameter space
#   )
# )
# Extract log-likelihood for PSIS-LOO cross-validation
log_lik <- extract_log_lik(fit, parameter_name = "log_lik", merge_chains = FALSE)
loo_result <- loo(log_lik, moment_match = TRUE)
print(loo_result)
# Posterior analysis of dynamic weights
bayesplot::mcmc_areas(as.array(fit), pars = c("ar_pos", "ar_neg", "ma_coef")) +
labs(title = "Posterior distributions of AR and MA coefficients")
bayesplot::mcmc_areas(as.array(fit), pars = c("sigma[1]", "sigma[2]", "sigma[3]", "sigma[4]")) +
labs(title = "Posterior distributions of state standard deviations")
bayesplot::mcmc_areas(as.array(fit), pars = c("mu[1]", "mu[2]", "mu[3]", "mu[4]", "lambda")) +
labs(title = "Posterior distributions of state means and smoothing parameter")
bayesplot::mcmc_areas(as.array(fit), pars = c("T[1,1]", "T[2,2]", "T[3,3]", "T[4,4]")) +
labs(title = "Posterior distributions of transition matrix diagonal elements")
# Check posterior distribution of transition matrix
print(fit, pars = "T")
# ===========================
#  Clear Workspace and Setup
# ===========================
rm(list = ls())  # Clear all objects from the workspace
# Set the working directory
setwd('C:/Users/kento/OneDrive - UT Cloud (1)/PhD/Ensemble/working')
# ===========================
#   Load External Scripts
# ===========================
source('library.R')       # Contains library imports and setups
source('DGP.R')           # Function for data generation process
source('fit_apriori.R')   # Model fitting function: Apriori
source('fit_BMA.R')       # Model fitting function: Bayesian Model Averaging
source('fit_BPS.R')       # Model fitting function: Bayesian Predictive Stacking
source('fit_BPS2.R')      # Model fitting function: Alternative Bayesian Predictive Stacking
library_load()  # Load libraries using custom function
# ===========================
#  Set Parameters
# ===========================
N <- 10
Nt <- 50
i <- 1
# ===========================
#   Multiple Runs Setup
# ===========================
n_runs <- 1          # Number of iterations
result_list <- list()  # Store results for each run
# ===========================
#   Model Fitting Parameters
# ===========================
n_iter <- 100   # Number of iterations for Stan model
n_chains <- 1    # Number of chains for Stan model
# Progress bar setup
pb <- txtProgressBar(min = 0, max = n_runs, style = 3)
# Update seed
seed <- 123 + i  # Change seed for each iteration
# Generate data
df <- DGP(N = N, Nt = Nt, seed = seed, train_ratio = 0.6, val_ratio = 0.2)
# Fit models
res_apriori <- fit_apriori(data = df, iter = n_iter, chains = n_chains, refresh = 0)
res_BPS <- fit_BPS(data = res_apriori$data_fit, iter = n_iter, chains = n_chains, refresh = 0)
res_BPS2 <- fit_BPS2(data = res_apriori$data_fit, iter = n_iter, chains = n_chains, refresh = 0)
